# ç¬¬ 1 ç« ï¼šæ‹¥æŠ± AMD AI ç®—åŠ›æ–°æ—¶ä»£

<div align='center'>

[![AMD](https://img.shields.io/badge/AMD-ROCm-ED1C24)](https://rocm.docs.amd.com/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.x-EE4C2C)](https://pytorch.org/)

</div>

---

## ğŸ¯ æœ¬ç« å­¦ä¹ ç›®æ ‡

æœ¬ç« çš„ç›®æ ‡ï¼Œæ˜¯å¸®ä½ ææ¸…æ¥šä¸‰ä»¶äº‹ï¼š

1. âœ… **ä½ çš„ AMD è®¾å¤‡èƒ½åšä»€ä¹ˆ**â€”â€”ä» Ryzen AI æœ¬åœ° NPUï¼Œåˆ° Radeon ç‹¬æ˜¾ã€Instinct åŠ é€Ÿå¡çš„ AI èƒ½åŠ›å…¨æ™¯å›¾
2. âœ… **ROCm æ˜¯ä»€ä¹ˆ**â€”â€”ä¸ºä»€ä¹ˆè¯´å®ƒæ˜¯ AI çš„"åŸºç¡€è®¾æ–½"è€Œä¸æ˜¯å•çº¯çš„"é©±åŠ¨"
3. âœ… **åŠ¨æ‰‹å®æˆ˜**â€”â€”åœ¨ AMD å¹³å°ä¸Šç”¨ PyTorch è·‘é€š ResNet è®­ç»ƒå’Œ Qwen 3 å¤§æ¨¡å‹æ¨ç†

***OKï¼Œé‚£æ¥ä¸‹æ¥æˆ‘å°†ä¼šå¸¦é¢†å¤§å®¶ä¸€æ­¥æ­¥æ¢ç´¢ AMD AI ç®—åŠ›æ–°æ—¶ä»£ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ¥ä½“éªŒä¸€ä¸‹å§~***

---

## 1.1 ä½ çš„ AMD æ˜¾å¡èƒ½åšä»€ä¹ˆï¼Ÿ

> ğŸ’¡ **å°è´´å£«**ï¼šè¿‡å»æˆ‘ä»¬è°ˆ AIï¼Œå‡ ä¹ç­‰äº"è‹±ä¼Ÿè¾¾ + CUDA"ã€‚ç°åœ¨è¿™ä¸ªæ ¼å±€å·²ç»è¢«æ‰“ç ´â€”â€”AMD ä»ä½åŠŸè€— AI PCï¼Œåˆ°æ¡Œé¢/å·¥ä½œç«™æ˜¾å¡ï¼Œå†åˆ°æ•°æ®ä¸­å¿ƒåŠ é€Ÿå¡ï¼Œå½¢æˆäº†ä¸€æ¡å®Œæ•´çš„ AI äº§å“çº¿ï¼Œå¹¶ä¸”ç»Ÿä¸€åœ¨ ROCm è½¯ä»¶æ ˆä¹‹ä¸‹ã€‚

### 1.1.1 ä» AI PC åˆ°ç‹¬ç«‹æ˜¾å¡çš„å…¨æ™¯å›¾

è¿‡å»æˆ‘ä»¬è°ˆ AIï¼Œå‡ ä¹ç­‰äºâ€œè‹±ä¼Ÿè¾¾ + CUDAâ€ã€‚ç°åœ¨è¿™ä¸ªæ ¼å±€å·²ç»è¢«æ‰“ç ´ï¼šAMD ä»ä½åŠŸè€— AI PCï¼Œåˆ°æ¡Œé¢/å·¥ä½œç«™æ˜¾å¡ï¼Œå†åˆ°æ•°æ®ä¸­å¿ƒåŠ é€Ÿå¡ï¼Œå½¢æˆäº†ä¸€æ¡å®Œæ•´çš„ AI äº§å“çº¿ï¼Œå¹¶ä¸”ç»Ÿä¸€åœ¨ ROCm è½¯ä»¶æ ˆä¹‹ä¸‹ã€‚

å¯ä»¥æŠŠ AMD çš„ AI ç¡¬ä»¶å¤§è‡´åˆ†æˆä¸‰æ¡£æ¥çœ‹ï¼š

### ğŸ–¥ï¸ 1ï¼‰AI PCï¼šRyzen AIï¼ˆNPU + GPUï¼‰

ä»¥ 2026 å¹´çš„ **Ryzen AI 400 ç³»åˆ—**ä¸ºä¾‹ï¼ŒNPU ç®—åŠ›æœ€é«˜å¯è¾¾ **60 TOPS**ï¼Œæ»¡è¶³ç”šè‡³è¶…è¿‡å¾®è½¯ Copilot+ PC çš„ 40 TOPS è¦æ±‚[1][2]ã€‚

ä¸€ä¸ªå…¸å‹çš„ Ryzen AI 400 èŠ¯ç‰‡å†…éƒ¨é€šå¸¸åŒ…å«ï¼š

| ç»„ä»¶ | åŠŸèƒ½ | å…¸å‹åº”ç”¨ |
| :--- | :--- | :--- |
| **Zen 5 CPU æ ¸å¿ƒ** | é€šç”¨è®¡ç®—ã€æ•°æ®é¢„å¤„ç† | æ•°æ®é¢„å¤„ç†ã€é€»è¾‘æ§åˆ¶ |
| **RDNA 3.5/4 é›†æˆ GPU** | ä¸­ç­‰è§„æ¨¡æ¨¡å‹è®­ç»ƒã€å°æ¨¡å‹æ¨ç† | 7B çº§åˆ«æ¨ç†ã€LoRA å¾®è°ƒã€å›¾åƒ/è§†é¢‘ç”Ÿæˆ |
| **XDNA 2 NPU** | é«˜æ•ˆæ‰§è¡Œæœ¬åœ° AI æ¨ç†ä»»åŠ¡ | è¯­éŸ³è¯†åˆ«ã€å®æ—¶ç¿»è¯‘ã€Copilot+ åŠŸèƒ½ |


### ğŸ® 2ï¼‰æ¡Œé¢/å·¥ä½œç«™ï¼šRadeon RX / Radeon Pro

å¯¹å¤§éƒ¨åˆ†å¼€å‘è€…æ¥è¯´ï¼Œå…¥é—¨å‹å¥½çš„é€‰æ‹©æ˜¯ **Radeon RX 7000 / 9000 ç³»åˆ—ï¼ˆRDNA 3 / RDNA 4ï¼‰**ï¼š

| ç±»å‹ | ä»£è¡¨å‹å· | ç‰¹ç‚¹ |
| :--- | :--- | :--- |
| **æ¸¸æˆå¡** | Radeon RX 7700ã€RX 9070 ç­‰ | æ€§ä»·æ¯”é«˜ï¼Œé€‚åˆå¼€å‘è€…å’Œä¸ªäººç”¨æˆ· |
| **ä¸“ä¸šå¡** | Radeon AI PRO / Radeon Pro W ç³»åˆ— | æ˜¾å­˜æ›´å¤§ã€æ›´ç¨³å®šï¼Œé€‚åˆä¸“ä¸šå·¥ä½œ |

#### ğŸš€ RDNA 4 AI åŠ é€Ÿäº®ç‚¹

RDNA 4 å¼•å…¥äº†æ›´å¼ºçš„ AI åŠ é€Ÿèƒ½åŠ›[3]ï¼š

- âœ¨ **æ¯ä¸ª Compute Unit å†…é›†æˆ 2 ä¸ª AI åŠ é€Ÿå™¨**
- ğŸ“ˆ **AI ç®—åŠ›æå‡è¶…è¿‡ 4 å€**ï¼ˆç›¸æ¯”ä¸Šä¸€ä»£ RDNA 3ï¼‰
- ğŸ’ª **åƒ TOPS ç­‰çº§è¿ç®—**ï¼ˆéƒ¨åˆ† 9000 ç³»åˆ—å¡ï¼Œæ­é… 16GB+ æ˜¾å­˜ï¼‰

#### ğŸ’¼ æ¡Œé¢/å·¥ä½œç«™å…¸å‹ç”¨æ³•

- ğŸ¨ æœ¬åœ° Stable Diffusion / ComfyUI å…¨æµç¨‹
- ğŸ¤– ä¸­ç­‰è§„æ¨¡ï¼ˆ7Bâ€“14Bï¼‰LLM æ¨ç†ä¸ LoRA å¾®è°ƒ
- ğŸ–¼ï¸ å›¾åƒåˆ†ç±»ã€æ£€æµ‹ã€åˆ†å‰²ç­‰è®­ç»ƒä»»åŠ¡

### ğŸ¢ 3ï¼‰æ•°æ®ä¸­å¿ƒï¼šInstinct MI ç³»åˆ—

å¦‚æœä½ åœ¨åšå¤§è§„æ¨¡è®­ç»ƒæˆ–éƒ¨ç½² 70B ç”šè‡³ 400B çº§åˆ«çš„æ¨¡å‹ï¼ŒAMD çš„ **Instinct MI300X / MI350X / MI355X** ç³»åˆ—æ˜¯ä¸»åŠ›ç¡¬ä»¶[4][5]ï¼š

#### ğŸ’ Instinct ç³»åˆ—æ ¸å¿ƒä¼˜åŠ¿

| ç‰¹æ€§ | è¯´æ˜ | åº”ç”¨ä»·å€¼ |
| :--- | :--- | :--- |
| **è¶…å¤§æ˜¾å­˜** | æœ€é«˜ **192GB HBM** é«˜å¸¦å®½æ˜¾å­˜ | æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡å¤§æ¨¡å‹ï¼ˆå¦‚ Qwen3-Coder-Next 80Bï¼‰ |
| **å…ˆè¿›ç²¾åº¦** | æ”¯æŒ **FP8 ç²¾åº¦ã€256k ä¸Šä¸‹æ–‡é•¿åº¦** | æ»¡è¶³æœ€æ–°ä»£ç æ¨¡å‹å’Œå¤šæ¨¡æ€æ¨¡å‹éœ€æ±‚[5] |
| **æ·±åº¦ä¼˜åŒ–** | ROCm 7 åœ¨ Llama 3.xã€GLMã€DeepSeek ç­‰æ¨¡å‹ä¸Šåšäº†ç®—å­çº§ä¼˜åŒ– | æ˜¾è‘—æå‡è®­ç»ƒä¸æ¨ç†åå[4] |

#### ğŸ­ ä½¿ç”¨åœºæ™¯

- ğŸ”¬ å¤§æ¨¡å‹è®­ç»ƒï¼ˆ70B+ï¼‰
- ğŸŒ å¤š GPU / å¤šèŠ‚ç‚¹æ¨ç†é›†ç¾¤
- ğŸ¢ ä¼ä¸šçº§ AI æœåŠ¡å¹³å°

---

### 1.1.2 ROCm ç”Ÿæ€ç°çŠ¶ï¼šå®ƒä¸ä»…ä»…æ˜¯"é©±åŠ¨"

> âš ï¸ **é‡è¦è®¤çŸ¥**ï¼šå¾ˆå¤šäººä»¥ä¸º"è£…ä¸Š ROCm å°±æ˜¯è£…äº†é©±åŠ¨"ï¼Œå…¶å® ROCm æ›´åƒæ˜¯ä¸€æ•´å¥—**å¼€æº AI è®¡ç®—å¹³å°**ï¼Œç±»ä¼¼"CUDA ç”Ÿæ€"çš„ AMD ç‰ˆæœ¬ã€‚

#### ğŸ”§ ROCm æ˜¯ä»€ä¹ˆï¼Ÿ

ROCmï¼ˆRadeon Open Computeï¼‰ä¸»è¦åŒ…å«å‡ å±‚ï¼š

```mermaid
graph TB
    subgraph "ğŸ› ï¸ å·¥å…·é“¾ä¸è°ƒè¯•"
        T1[profiling]
        T2[æ€§èƒ½è°ƒä¼˜]
        T3[ç›‘æ§å·¥å…·<br/>rocm-smi]
    end

    subgraph "ğŸ¤– AI æ¡†æ¶é€‚é…"
        F1[PyTorch on ROCm]
        F2[TensorFlow/XLA]
    end

    subgraph "ğŸ“š æ·±åº¦å­¦ä¹ åº“"
        L1[MIOpen<br/>å·ç§¯/æ± åŒ–/RNN<br/>ç±»ä¼¼ cuDNN]
        L2[RCCL<br/>å¤š GPU é€šä¿¡<br/>ç±»ä¼¼ NCCL]
    end

    subgraph "ğŸ’» ç¼–ç¨‹æ¥å£å±‚"
        H[HIP<br/>CUDA ç±»ä¼¼ C++ æ¥å£<br/>ä¾¿äºä»£ç è¿ç§»]
    end

    subgraph "ğŸ”§ é©±åŠ¨å±‚"
        D[amdgpu å†…æ ¸é©±åŠ¨<br/>è®©ç³»ç»Ÿè¯†åˆ« GPU]
    end

    T1 --> F1
    T2 --> F1
    T3 --> F1
    F1 --> L1
    F1 --> L2
    F2 --> L1
    F2 --> L2
    L1 --> H
    L2 --> H
    H --> D

    style D fill:#e1f5ff
    style H fill:#fff3e0
    style L1 fill:#f3e5f5
    style L2 fill:#f3e5f5
    style F1 fill:#e8f5e9
    style F2 fill:#e8f5e9
```

> ğŸ’¡ **ä¸€å¥è¯æ€»ç»“**ï¼š**ROCm = AMD ç‰ˆçš„ CUDA ç”Ÿæ€ + è¿˜æ›´å¼€æ”¾**

#### ğŸŒŸ ROCm 7.2 çš„å‡ ä¸ªå…³é”®ç‚¹

æ ¹æ® 2026 å¹´çš„å®˜æ–¹ä¿¡æ¯å’Œåª’ä½“æŠ¥é“[1][4][6]ï¼ŒROCm 7.2 æœ‰å‡ ä¸ªå¯¹å¼€å‘è€…å¾ˆé‡è¦çš„å˜åŒ–ï¼š

| # | ç‰¹æ€§ | è¯´æ˜ |
| :--- | :--- | :--- |
| **1** | **ğŸªŸğŸ§ åŒå¹³å°æ­£å¼æ”¯æŒ** | Windowsï¼ˆAdrenalin 26.1.1ï¼‰+ Linuxï¼ˆUbuntu ç­‰ï¼‰ä¸€é”®å®‰è£… |
| **2** | **ğŸ¯ æ”¯æŒé¢æ‰©å±•åˆ°æ¶ˆè´¹çº§** | ä¸å†å±€é™æ•°æ®ä¸­å¿ƒï¼Œæ­£å¼æ”¯æŒ Radeon RX 7000/9000 + Ryzen AI 300/400 |
| **3** | **âš¡ ä¸º PyTorch æ·±åº¦ä¼˜åŒ–** | Llamaã€GLMã€DeepSeek ç­‰æ¨¡å‹å†…æ ¸çº§ä¼˜åŒ–ï¼Œ"è£…å®Œå°±èƒ½ç”¨" |
| **4** | **ğŸ¤ ä¸ Ubuntu æ·±åº¦é›†æˆ** | Ubuntu 26.04 LTS èµ·åŸç”Ÿæ”¯æŒï¼Œé•¿æœŸç¨³å®šçš„ AI ç¯å¢ƒ[7] |

---

## 1.2 PyTorch on ROCmï¼šæ— ç¼è¡”æ¥

è¿™ä¸€å°èŠ‚ä¸“æ³¨åœ¨ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š

| é—®é¢˜ | è¯´æ˜ |
| :--- | :--- |
| **ğŸ“¦ æ€ä¹ˆè£…ï¼Ÿ** | pip install èƒŒåçš„ç‰ˆæœ¬é€‰æ‹©ï¼ˆç¨³å®šç‰ˆ / Nightly / Windowsï¼‰ |
| **âœ… çœŸçš„å…¼å®¹å—ï¼Ÿ** | ä¸ºä»€ä¹ˆåœ¨ AMD ä¸Š `torch.cuda.is_available()` ä¹Ÿæ˜¯ True |
| **ğŸš€ è·‘å¾—åŠ¨å•¥ï¼Ÿ** | å®æˆ˜ï¼šResNet è®­ç»ƒ Demo + Qwen 3 æ¨ç† Demo |

---

### 1.2.1 å®‰è£…ï¼špip install é‡Œçš„ç„æœºï¼ˆå®˜æ–¹ / nightly å¦‚ä½•é€‰ï¼‰

#### ğŸ“Š ç‰ˆæœ¬åˆ†å±‚æ¦‚è§ˆ

PyTorch on ROCm çš„åŒ…ï¼Œé€šå¸¸å¯ä»¥åˆ†æˆä¸‰ä¸ªå±‚æ¬¡ï¼š

#### ğŸ”µ 1. ç¨³å®šç‰ˆï¼ˆStableï¼‰- AMD å®˜æ–¹æ¨è

> ğŸ’¡ **é‡è¦è¯´æ˜**ï¼šAMD æ¨èä½¿ç”¨ **repo.radeon.com** çš„ ROCm WHL æ–‡ä»¶ï¼Œè€Œé PyTorch.org çš„ç‰ˆæœ¬ï¼ˆåè€…æœªç» AMD å……åˆ†æµ‹è¯•ï¼‰ã€‚

##### ğŸ“‹ å‰ç½®æ¡ä»¶ [8]

- Python 3.12 ç¯å¢ƒ
- Ubuntu 24.04 / 22.04

##### å®‰è£…æ­¥éª¤

**æ­¥éª¤ 1ï¼šæ›´æ–° pip**

```bash
# å®‰è£… pipï¼ˆå¦‚æœå°šæœªå®‰è£…ï¼‰
sudo apt install python3-pip -y

# æ›´æ–° pip å’Œ wheel
pip3 install --upgrade pip wheel
```

**æ­¥éª¤ 2ï¼šä¸‹è½½å¹¶å®‰è£… PyTorch for ROCm**

Ubuntu 22.04 ç¤ºä¾‹ï¼š

```bash
# ä¸‹è½½ WHL æ–‡ä»¶
wget https://repo.radeon.com/rocm/manylinux/rocm-rel-7.2/torch-2.9.1%2Brocm7.2.0.lw.git7e1940d4-cp312-cp312-linux_x86_64.whl
wget https://repo.radeon.com/rocm/manylinux/rocm-rel-7.2/torchvision-0.24.0%2Brocm7.2.0.gitb919bd0c-cp312-cp312-linux_x86_64.whl
wget https://repo.radeon.com/rocm/manylinux/rocm-rel-7.2/triton-3.5.1%2Brocm7.2.0.gita272dfa8-cp312-cp312-linux_x86_64.whl
wget https://repo.radeon.com/rocm/manylinux/rocm-rel-7.2/torchaudio-2.9.0%2Brocm7.2.0.gite3c6ee2b-cp312-cp312-linux_x86_64.whl

# å¸è½½æ—§ç‰ˆæœ¬ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
pip3 uninstall torch torchvision triton torchaudio

# å®‰è£…æ–°ç‰ˆæœ¬
pip3 install torch-2.9.1+rocm7.2.0.lw.git7e1940d4-cp312-cp312-linux_x86_64.whl \
  torchvision-0.24.0+rocm7.2.0.gitb919bd0c-cp312-cp312-linux_x86_64.whl \
  torchaudio-2.9.0+rocm7.2.0.gite3c6ee2b-cp312-cp312-linux_x86_64.whl \
  triton-3.5.1+rocm7.2.0.gita272dfa8-cp312-cp312-linux_x86_64.whl
```

> âš ï¸ **æ³¨æ„**ï¼šåœ¨éè™šæ‹Ÿç¯å¢ƒçš„ Python 3.12 ä¸­å®‰è£…æ—¶ï¼Œå¿…é¡»æ·»åŠ  `--break-system-packages` æ ‡å¿—ã€‚

**æ­¥éª¤ 3ï¼šéªŒè¯å®‰è£…**

```bash
# éªŒè¯ PyTorch æ˜¯å¦æ­£ç¡®å®‰è£…
python3 -c 'import torch' 2> /dev/null && echo 'Success' || echo 'Failure'

# éªŒè¯ GPU æ˜¯å¦å¯ç”¨
python3 -c 'import torch; print(torch.cuda.is_available())'

# æ˜¾ç¤º GPU è®¾å¤‡åç§°
python3 -c "import torch; print(f'device name [0]:', torch.cuda.get_device_name(0))"

# æ˜¾ç¤ºå®Œæ•´çš„ PyTorch ç¯å¢ƒä¿¡æ¯
python3 -m torch.utils.collect_env
```

**é¢„æœŸè¾“å‡º**ï¼š

```
Success
True
device name [0]: AMD Radeon 8060S  # æˆ–å…¶ä»–æ”¯æŒçš„ AMD GPU
```

> âœ… **é€‚ç”¨åœºæ™¯**ï¼šç”Ÿäº§ç¯å¢ƒå’Œæ—¥å¸¸è®­ç»ƒï¼ˆAMD å®˜æ–¹æ¨èï¼‰

#### ğŸŸ¡ 2. Docker å®‰è£…ï¼ˆå¯é€‰ï¼‰

ä½¿ç”¨ Docker å¯ä»¥æä¾›æ›´å¥½çš„å¯ç§»æ¤æ€§å’Œé¢„æ„å»ºçš„å®¹å™¨ç¯å¢ƒã€‚

**å®‰è£… Docker**ï¼š

```bash
sudo apt install docker.io
```

**æ‹‰å–å¹¶è¿è¡Œ PyTorch Docker é•œåƒ**ï¼ˆUbuntu 24.04ï¼‰ï¼š

```bash
# æ‹‰å–é•œåƒ
sudo docker pull rocm/pytorch:rocm7.2_ubuntu24.04_py3.12_pytorch_release_2.9.1

# å¯åŠ¨å®¹å™¨
sudo docker run -it \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --device=/dev/kfd \
  --device=/dev/dri \
  --group-add video \
  --ipc=host \
  --shm-size 8G \
  rocm/pytorch:rocm7.2_ubuntu24.04_py3.12_pytorch_release_2.9.1
```

> ğŸ’¡ **æç¤º**ï¼šå¯ä»¥ä½¿ç”¨ `-v` å‚æ•°æŒ‚è½½ä¸»æœºçš„æ•°æ®ç›®å½•åˆ°å®¹å™¨ä¸­ã€‚

#### ğŸ”´ 3. Windows ä¸“ç”¨ ROCm SDK è½®å­

å¯¹äº PyTorch on Windows + ROCm 7.2ï¼ŒAMD å®˜æ–¹æä¾›äº†å®Œæ•´ wheel é“¾æ¥[9]ï¼š
     - å…ˆå®‰è£… ROCm SDK ç»„ä»¶ï¼ˆPython 3.12 ç¯å¢ƒï¼‰ï¼›
     - å†å®‰è£…å¸¦ `+rocmsdk20260116` æ ‡è®°çš„ torch/torchvision/torchaudio è½®å­ã€‚
   - å…¸å‹å‘½ä»¤ï¼ˆCMD ç¤ºä¾‹ï¼‰ï¼š
     ```bat
     pip install --no-cache-dir ^
       https://repo.radeon.com/rocm/windows/rocm-rel-7.2/torch-2.9.1%2Brocmsdk20260116-cp312-cp312-win_amd64.whl ^
       https://repo.radeon.com/rocm/windows/rocm-rel-7.2/torchaudio-2.9.1%2Brocmsdk20260116-cp312-cp312-win_amd64.whl ^
       https://repo.radeon.com/rocm/windows/rocm-rel-7.2/torchvision-0.24.1%2Brocmsdk20260116-cp312-cp312-win_amd64.whl
     ```

#### ğŸ¯ å¦‚ä½•é€‰æ‹©å®‰è£…æ–¹å¼ï¼Ÿ

| ä½ çš„éœ€æ±‚ | æ¨èæ–¹æ¡ˆ | è¯´æ˜ |
| :--- | :--- | :--- |
| **ğŸ”’ è¿½æ±‚ç¨³å®šï¼ˆLinuxï¼‰** | **repo.radeon.com WHL æ–‡ä»¶** | AMD å®˜æ–¹æ¨èï¼Œç»è¿‡å……åˆ†æµ‹è¯• |
| **ğŸ³ å¿«é€Ÿéƒ¨ç½²** | **Docker é•œåƒ** | é¢„æ„å»ºç¯å¢ƒï¼Œå¼€ç®±å³ç”¨ï¼Œè·¨å¹³å° |
| **ğŸš€ æ–°ç¡¬ä»¶å°é²œ** | **Nightly ROCm è½®å­** | æ–°ç¡¬ä»¶æ”¯æŒ + æ–°åŠŸèƒ½ï¼Œèƒ½æ¥å—å¶å°”è¸©å‘ |
| **ğŸªŸ Windows ç”¨æˆ·** | **AMD å®˜æ–¹ ROCm SDK** | Windows + Radeon + Ryzen AI ç¯å¢ƒ |

---

### 1.2.2 å…¼å®¹æ€§æ­ç§˜ï¼š`torch.cuda.is_available()` åœ¨ AMD ä¸Šä¹Ÿæ˜¯ Trueï¼Ÿ

> ğŸ” **ç°è±¡æ­ç§˜**ï¼šå¾ˆå¤šäººç¬¬ä¸€æ¬¡åœ¨ AMD GPU ä¸Šè£…å¥½ PyTorch åï¼Œè¿è¡ŒéªŒè¯ä»£ç å‘ç° `torch.cuda.is_available()` è¿”å›çš„æ˜¯ **True**ã€‚è¿™ä¸æ˜¯ bugï¼Œè€Œæ˜¯ **å…¼å®¹æ€§è®¾è®¡**ã€‚

å¾ˆå¤šäººç¬¬ä¸€æ¬¡åœ¨ AMD GPU ä¸Šè£…å¥½ PyTorch åï¼Œè·‘ï¼š

```python
import torch
print(torch.cuda.is_available())
print(torch.cuda.get_device_name(0))
print(torch.version.hip)
```

**ç»“æœå‘ç°**ï¼š

- `torch.cuda.is_available()` è¿”å›çš„å±…ç„¶æ˜¯ **True**ï¼›
- `torch.cuda.get_device_name(0)` æ˜¾ç¤ºçš„æ˜¯ **Radeon RX 9070 XT**ã€**Radeon PRO W7900** æˆ– **Instinct MI300X** ç­‰ï¼›
- `torch.version.hip` æ˜¾ç¤ºç±»ä¼¼ `7.2.26015-fc0010cf6a`ã€‚

è¿™ä¸æ˜¯ bugï¼Œè€Œæ˜¯ **å…¼å®¹æ€§è®¾è®¡**ï¼š

<div style="background: #fff3cd; border: 1px solid #ffc107; border-radius: 8px; padding: 16px; margin: 16px 0;">
  <div style="display: flex; align-items: start;">
    <span style="font-size: 20px; margin-right: 10px;">âš¡</span>
    <div>
      <strong style="color: #856404;">ä¸ºä»€ä¹ˆ torch.cuda åœ¨ AMD ä¸Šä¹Ÿæ˜¯ Trueï¼Ÿ</strong><br>
      <span style="color: #856404; line-height: 1.6;">
        â€¢ PyTorch ç”Ÿæ€ï¼ˆhuggingface ç­‰ï¼‰ä¾èµ– <code>torch.cuda.*</code> API åˆ¤æ–­ GPU<br>
        â€¢ ä¸ºäº†å…¼å®¹æ€§ï¼ŒROCm åç«¯æ²¿ç”¨äº† <code>cuda</code> å‘½åç©ºé—´<br>
        â€¢ åº•å±‚å®é™…è¿è¡Œçš„æ˜¯ <strong>HIP/ROCm</strong>ï¼Œæ— éœ€ä¿®æ”¹ä»»ä½•ä»£ç  âœ¨
      </span>
    </div>
  </div>
</div>


> âœ… **ç»“è®º**ï¼šåœ¨ AMD å¹³å°ä¸Šï¼š
> - `torch.cuda.*` â‰ˆ "æœ‰ GPU åŠ é€Ÿï¼Œåº•å±‚æ˜¯ ROCm/HIP"
> - `torch.version.rocm` æ‰æ˜¯ä½ çœŸæ­£æŸ¥çœ‹ ROCm ç‰ˆæœ¬çš„åœ°æ–¹

---

### 1.2.3 å®æˆ˜ 1ï¼šResNet å›¾åƒåˆ†ç±»è®­ç»ƒ Demo

> ğŸ¯ **å®æˆ˜ç›®æ ‡**ï¼šä¸‹é¢ç»™å‡ºä¸€ä¸ªå¯ä»¥ç›´æ¥åœ¨ AMD GPU ä¸Šè·‘çš„ **ResNet18 + CIFAR10** è®­ç»ƒ Demoã€‚ä»£ç é€»è¾‘å‚è€ƒäº† AMD å®˜æ–¹çš„ ROCm åšå®¢ç¤ºä¾‹[10]ï¼Œç¨ä½œç²¾ç®€å’Œæ³¨é‡Šã€‚

#### ğŸ“¦ ç¯å¢ƒå‡†å¤‡

ç¡®ä¿ä½ å·²ç»åœ¨å½“å‰ Python ç¯å¢ƒä¸­ï¼š

- âœ… è£…å¥½äº†æ”¯æŒ ROCm çš„ PyTorch
- âœ… å®‰è£…äº†ä»¥ä¸‹ä¾èµ–ï¼š

```bash
pip install torchvision datasets matplotlib
```

#### ğŸ’» å®Œæ•´è®­ç»ƒè„šæœ¬ç¤ºä¾‹

```python
# file: resnet_cifar10_amd.py
import random
import datetime
import torch
import torchvision
from datasets import load_dataset
import matplotlib.pyplot as plt


def get_dataloaders(batch_size=256):
    dataset = load_dataset("cifar10")
    dataset.set_format("torch")

    train_loader = torch.utils.data.DataLoader(
        dataset["train"], shuffle=True, batch_size=batch_size
    )
    test_loader = torch.utils.data.DataLoader(
        dataset["test"], batch_size=batch_size
    )
    return train_loader, test_loader


def get_transform():
    mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(1, 3, 1, 1)
    std = torch.tensor([0.2023, 0.1994, 0.2010]).view(1, 3, 1, 1)

    def transform(x):
        # x å¯èƒ½æ˜¯ (B, H, W, C) æˆ– (B, C, H, W)
        if x.ndim == 4 and x.shape[1] != 3:
            x = x.permute(0, 3, 1, 2)  # ä»…åœ¨ BHWC æ—¶è½¬æ¢
        x = x.float() / 255.0
        x = (x - mean.to(x.device)) / std.to(x.device)
        return x

    return transform


def build_model():
    model = torchvision.models.resnet18(num_classes=10)
    loss_fn = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)
    return model, loss_fn, optimizer


def train_model(model, loss_fn, optimizer, train_loader, test_loader, transform, num_epochs):
    print(f"Number of GPUs: {torch.cuda.device_count()}")
    print([torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())])
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model.to(device)
    accuracy = []
    t0 = datetime.datetime.now()

    for epoch in range(num_epochs):
        print(f"Epoch {epoch+1}/{num_epochs}")
        t0_epoch_train = datetime.datetime.now()

        model.train()
        train_losses, n_examples = [], 0
        for batch in train_loader:
            batch = {k: v.to(device) for k, v in batch.items()}

            optimizer.zero_grad()
            preds = model(transform(batch["img"]))
            loss = loss_fn(preds, batch["label"])
            loss.backward()
            optimizer.step()

            train_losses.append(loss.detach())
            n_examples += batch["label"].shape[0]

        train_loss = torch.stack(train_losses).mean().item()
        t_epoch_train = datetime.datetime.now() - t0_epoch_train

        model.eval()
        with torch.no_grad():
            t0_epoch_test = datetime.datetime.now()
            test_losses, n_test_examples, n_test_correct = [], 0, 0
            for batch in test_loader:
                batch = {k: v.to(device) for k, v in batch.items()}

                preds = model(transform(batch["img"]))
                loss = loss_fn(preds, batch["label"])

                test_losses.append(loss)
                n_test_examples += batch["img"].shape[0]
                n_test_correct += (batch["label"] == preds.argmax(dim=1)).sum()

            test_loss = torch.stack(test_losses).mean().item()
            test_accuracy = n_test_correct / n_test_examples
            t_epoch_test = datetime.datetime.now() - t0_epoch_test
            accuracy.append(test_accuracy.cpu())

        print(f"  Epoch time: {t_epoch_train+t_epoch_test}")
        print(f"  Examples/second (train): {n_examples/t_epoch_train.total_seconds():0.4g}")
        print(f"  Examples/second (test): {n_test_examples/t_epoch_test.total_seconds():0.4g}")
        print(f"  Train loss: {train_loss:0.4g}")
        print(f"  Test loss: {test_loss:0.4g}")
        print(f"  Test accuracy: {test_accuracy*100:0.4g}%")

    total_time = datetime.datetime.now() - t0
    print(f"Total training time: {total_time}")
    return accuracy


def main():
    torch.manual_seed(0)
    random.seed(0)

    model, loss, optimizer = build_model()
    train_loader, test_loader = get_dataloaders()
    transform = get_transform()

    test_accuracy = train_model(
        model, loss, optimizer, train_loader, test_loader, transform, num_epochs=8
    )

    plt.plot(test_accuracy)
    plt.xlabel("Epoch")
    plt.ylabel("Test Accuracy")
    plt.title("ResNet18 on CIFAR10 (AMD ROCm)")
    plt.savefig("resnet_cifar10_amd.png")
    print("è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡æ›²çº¿å·²ä¿å­˜ä¸º resnet_cifar10_amd.png")


if __name__ == "__main__":
    main()

```

#### ğŸš€ è¿è¡Œæ–¹å¼

```bash
python resnet_cifar10_amd.py
```

#### âœ… è¾“å‡º

![alt text](images/resnet_cifar10.png)

![alt text](images/resnet_cifar10_amd.png)

---

### 1.2.4 å®æˆ˜ 2ï¼šQwen 2.5 æ¨¡å‹æ¨ç† Demoï¼ˆvLLM + ROCmï¼‰

> ğŸš€ **å®æˆ˜ç›®æ ‡**ï¼šæœ¬èŠ‚å±•ç¤ºå¦‚ä½•åœ¨ AMD GPU ä¸Šé€šè¿‡ **vLLM + ROCm 7** è¿è¡Œé˜¿é‡Œ Qwen2.5 ç³»åˆ—å¤§æ¨¡å‹çš„æ¨ç†ã€‚
>
> ğŸ’¡ **é€‚ç”¨æç¤º**ï¼šæœ¬ç¤ºä¾‹ä»¥ Qwen2.5-7B-Instruct ä¸ºä¾‹ï¼Œé€‚åˆæ¡Œé¢ Radeon å’Œæ•°æ®ä¸­å¿ƒ Instinct ç³»åˆ— GPUã€‚

#### æ­¥éª¤ 1ï¼šä½¿ç”¨ Docker å¯åŠ¨ vLLM ç¯å¢ƒ

ä½¿ç”¨ Docker å¯ä»¥å¿«é€Ÿè·å¾—ä¸€ä¸ªé¢„é…ç½®å¥½çš„ vLLM + ROCm ç¯å¢ƒï¼š

```bash
docker run -it \
  --network=host \
  --device=/dev/kfd \
  --device=/dev/dri \
  --group-add=video \
  --ipc=host \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --shm-size 8G \
  -v $(pwd):/workspace \
  --name vllm \
  rocm/vllm-dev:rocm7.2_navi_ubuntu24.04_py3.12_pytorch_2.9_vllm_0.14.0rc0
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | è¯´æ˜ |
| :--- | :--- |
| `--network=host` | ä½¿ç”¨ä¸»æœºç½‘ç»œï¼Œä¾¿äºè®¿é—®æœåŠ¡ |
| `--device=/dev/kfd --device=/dev/dri` | æŒ‚è½½ GPU è®¾å¤‡ |
| `--group-add=video` | æ·»åŠ åˆ° video ç»„ä»¥è®¿é—® GPU |
| `--ipc=host --shm-size 8G` | å…±äº«å†…å­˜é…ç½®ï¼Œç”¨äºå¤šè¿›ç¨‹é€šä¿¡ |
| `-v $(pwd):/workspace` | æŒ‚è½½å½“å‰ç›®å½•åˆ°å®¹å™¨çš„ /workspace |

#### æ­¥éª¤ 2ï¼šç¯å¢ƒå‡†å¤‡

è¿›å…¥å®¹å™¨åï¼Œå®‰è£…åŸºç¡€åº“ï¼š

```bash
pip install transformers accelerate
```

#### æ­¥éª¤ 3ï¼šä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨ ModelScopeï¼‰

å®‰è£… ModelScopeï¼š

```bash
pip install modelscope
```

åœ¨ç»ˆç«¯è¾“å…¥ `python` è¿›å…¥äº¤äº’æ¨¡å¼ï¼š

```python
from modelscope import snapshot_download

# ä¸‹è½½åˆ°å½“å‰ç›®å½•
model_dir = snapshot_download('Qwen/Qwen2.5-7B-Instruct', cache_dir='./')
print(f"æ¨¡å‹å·²ä¸‹è½½åˆ°: {model_dir}")
```
**è¾“å‡ºç¤ºä¾‹ï¼š**
```
æ¨¡å‹å·²ä¸‹è½½åˆ°: ./Qwen/Qwen2___5-7B-Instructors
```

#### æ­¥éª¤ 4ï¼šå¯åŠ¨ vLLM æ¨ç†æœåŠ¡

```bash
python -m vllm.entrypoints.openai.api_server \
  --model ./Qwen/Qwen2___5-7B-Instruct \
  --host 0.0.0.0 \
  --port 3000 \
  --dtype float16 \
  --gpu-memory-utilization 0.9 \
  --swap-space 16 \
  --disable-log-requests \
  --tensor-parallel-size 1 \
  --max-num-seqs 64 \
  --max-num-batched-tokens 32768 \
  --max-model-len 32768 \
  --distributed-executor-backend mp
```

**å‚æ•°è¯´æ˜**ï¼š

| å‚æ•° | è¯´æ˜ |
| :--- | :--- |
| `--model` | æ¨¡å‹è·¯å¾„ |
| `--dtype float16` | ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•° |
| `--gpu-memory-utilization 0.9` | GPU æ˜¾å­˜åˆ©ç”¨ç‡ |
| `--swap-space 16` | Swap ç©ºé—´å¤§å°ï¼ˆGBï¼‰ |
| `--max-model-len 32768` | æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ |

#### æ­¥éª¤ 5ï¼šæµ‹è¯•æ¨ç†æœåŠ¡

ä½¿ç”¨ curl å‘é€è¯·æ±‚ï¼š

```bash
curl -s http://127.0.0.1:3000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "./Qwen/Qwen2___5-7B-Instruct",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ Qwen2.5-7B-Instructã€‚"}
    ],
    "temperature": 0.7,
    "max_tokens": 256
  }' | jq .
```

#### âœ… é¢„æœŸç»“æœ

å¦‚æœä¸€åˆ‡æ­£å¸¸ï¼Œä½ ä¼šæ”¶åˆ°ç±»ä¼¼ä»¥ä¸‹çš„ JSON å“åº”ï¼ŒåŒ…å« Qwen2.5 æ¨¡å‹ç”Ÿæˆçš„å›ç­”ï¼š
![alt text](images/Qwen2.5_vllm.png)

---

### 1.2.5 å®æˆ˜ 3ï¼šQwen 2.5 åŸç”Ÿ PyTorch æ¨ç†

> ğŸ¯ **å®æˆ˜ç›®æ ‡**ï¼šæœ¬èŠ‚å±•ç¤ºå¦‚ä½•ä¸ä¾èµ– vLLM ç­‰æ¨ç†æ¡†æ¶ï¼Œç›´æ¥ä½¿ç”¨ **PyTorch + Transformers** åœ¨ AMD GPU ä¸Šè¿è¡Œ Qwen2.5 æ¨¡å‹æ¨ç†ã€‚
>
> ğŸ’¡ **é€‚ç”¨åœºæ™¯**ï¼šéœ€è¦æ›´çµæ´»çš„æ§åˆ¶ã€ç ”ç©¶æ¨¡å‹å†…éƒ¨è¡Œä¸ºã€æˆ–åªéœ€ç®€å•å•å¡æ¨ç†çš„åœºæ™¯ã€‚

#### æ­¥éª¤ 1ï¼šç¯å¢ƒå‡†å¤‡

ç¡®ä¿å·²å®‰è£…å¿…è¦çš„ä¾èµ–ï¼š

```bash
pip install torch transformers accelerate
```

#### æ­¥éª¤ 2ï¼šåˆ›å»ºæ¨ç†è„šæœ¬

åˆ›å»ºæ–‡ä»¶ `qwen_pytorch_inference.py`ï¼š

```python
# file: qwen_pytorch_inference.py
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer

# ==========================================
# æ ¸å¿ƒé…ç½®åŒº
# ==========================================

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "./Qwen/Qwen2___5-7B-Instruct"

# è®¾å¤‡é€‰æ‹©
DEVICE = "cuda:0"

# ==========================================

def run_inference():
    print(f"=== AMD ROCm PyTorch æ¨ç†æµ‹è¯• ===")

    # æ‰“å°è®¾å¤‡ä¿¡æ¯
    if torch.cuda.is_available():
        props = torch.cuda.get_device_properties(0)
        print(f"ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0)} ({props.total_memory / 1024**3:.1f} GB)")
    else:
        print("[è­¦å‘Š] æœªæ£€æµ‹åˆ° ROCm/CUDA è®¾å¤‡ï¼Œå°†ä½¿ç”¨ CPU è¿è¡Œï¼ˆææ…¢ï¼‰")

    # åŠ è½½ Tokenizer
    print("\n[1/3] æ­£åœ¨åŠ è½½ Tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, local_files_only=True,trust_remote_code=True)
    except Exception as e:
        print(f"[é”™è¯¯] Tokenizer åŠ è½½å¤±è´¥: {e}")
        return

    print("\n[2/3] æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡ (BFloat16)...")
    st = time.time()
    try:
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.bfloat16,  # AMD MIç³»åˆ—/æ–°å¡æ¨è BF16
            device_map=DEVICE,
            trust_remote_code=True,
        )
    except Exception as e:
        print(f"[è‡´å‘½é”™è¯¯] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("å¦‚æœæ˜¯æ˜¾å­˜ä¸è¶³ï¼Œè¯·å°è¯•ä½¿ç”¨é‡åŒ–æ¨¡å‹ã€‚")
        return

    print(f"æ¨¡å‹åŠ è½½è€—æ—¶: {time.time() - st:.2f} ç§’")

    # æ„å»ºå¯¹è¯
    prompt = "ä½ å¥½ï¼Œè¯·ç”¨è¿™å°é«˜æ€§èƒ½æ˜¾å¡ä¸ºæˆ‘å†™ä¸€é¦–å…³äº AMD æ˜¾å¡é€†è¢­çš„ä¸ƒè¨€ç»å¥ã€‚"
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ‰åæ¨ªæº¢çš„è¯—äººã€‚"},
        {"role": "user", "content": prompt}
    ]

    print("\n[3/3] å¼€å§‹æ¨ç†...")

    # åº”ç”¨èŠå¤©æ¨¡æ¿
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # ç¼–ç è¾“å…¥
    model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)

    # ç”Ÿæˆæ–‡æœ¬
    st = time.time()
    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    et = time.time()

    # è§£ç è¾“å‡º
    input_len = model_inputs.input_ids.shape[1]
    output_ids = generated_ids[:, input_len:]

    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

    # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
    tokens_gen = output_ids.shape[1]
    speed = tokens_gen / (et - st)

    print("\n" + "="*20 + " ç”Ÿæˆç»“æœ " + "="*20)
    print(response)
    print("="*50)
    print(f"ç”Ÿæˆé€Ÿåº¦: {speed:.2f} tokens/s")
    print(f"æ˜¾å­˜å ç”¨: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")

if __name__ == "__main__":
    # å¯ç”¨å®éªŒæ€§ ROCm ä¼˜åŒ–
    import os
    os.environ["TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL"] = "1"
    run_inference()
```

#### æ­¥éª¤ 3ï¼šè¿è¡Œæ¨ç†

```bash
python qwen_pytorch_inference.py
```

#### âœ… é¢„æœŸè¾“å‡º

![alt text](images/Qwen2.5_torch.png)

## ğŸ“– å‚è€ƒæ–‡çŒ®

| # | æè¿° | é“¾æ¥ |
| :--- | :--- | :--- |
| [1] | AMD ROCm 7.2 æ­£å¼å‘å¸ƒ:æ”¯æŒå¤šæ¬¾æ–°ç¡¬ä»¶,ä¼˜åŒ– Instinct AI æ€§èƒ½ | [é“¾æ¥](https://so.html5.qq.com/page/real/search_news?docid=70000021_7796976caaa35752) |
| [2] | AMD Expands AI Leadership Across Client, Graphics, and ... | [é“¾æ¥](https://www.amd.com/en/newsroom/press-releases/2026-1-5-amd-expands-ai-leadership-across-client-graphics-.html) |
| [3] | AI Acceleration with AMD Radeonâ„¢ Graphics Cards | [é“¾æ¥](https://www.amd.com/en/products/graphics/radeon-ai.html) |
| [4] | AMD ROCm 7.2 æ›´æ–°ç›¸å…³æŠ¥é“ï¼ˆITä¹‹å®¶ç­‰ç»¼åˆï¼‰ | [é“¾æ¥](https://so.html5.qq.com/page/real/search_news?docid=70000021_9816977467427752) |
| [5] | Day 0 Support for Qwen3-Coder-Next on AMD Instinct GPUs | [é“¾æ¥](https://www.amd.com/en/developer/resources/technical-articles/2026/day-0-support-for-qwen3-coder-next-on-amd-instinct-gpus.html) |
| [6] | ROCm 7 è½¯ä»¶ | [é“¾æ¥](https://www.amd.com/zh-cn/products/software/rocm/whats-new.html) |
| [7] | Ubuntu å°†åŸç”Ÿæ”¯æŒ AMD ROCm è½¯ä»¶ | [é“¾æ¥](https://so.html5.qq.com/page/real/search_news?docid=70000021_494693a705e92252) |
| [8] | Install PyTorch via PIP (Linux ROCm) | [é“¾æ¥](https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/docs/install/installrad/native_linux/install-pytorch.html) |
| [9] | Install PyTorch via PIP (Windows ROCm) | [é“¾æ¥](https://rocm.docs.amd.com/projects/radeon-ryzen/en/latest/docs/install/installrad/windows/install-pytorch.html) |
| [10] | ResNet for image classification using AMD GPUs | [é“¾æ¥](https://rocm.blogs.amd.com/artificial-intelligence/resnet/README.html) |
